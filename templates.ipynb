{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6465b661",
   "metadata": {},
   "source": [
    "ML End Module Exam\n",
    "\n",
    "Pandas Data Cleaning-\n",
    "One dataframe given,\n",
    "Ek column leke standard scalar kaise karna hai, get dummies kaise karna hai\n",
    "Ek que data pe rahega jispe classification algo use karna hai\n",
    "Preferred- use grid search series \n",
    "\n",
    "Regression ka ek problem\n",
    "You can do regression using Neural Network \n",
    "SImple neural network dense layer lagake\n",
    "Jitne bhi dense layer banaoge, output layer me 1 neuron lena hai aur \n",
    "Loss should be mean squared error, mae, model.compile\n",
    "\n",
    "Day 13 keras simple neural .ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20af360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()  # for plot styling\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc5f933",
   "metadata": {},
   "source": [
    "# Agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba15f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Draw Dendogram\n",
    "import scipy.cluster.hierarchy as shc\n",
    "plt.figure(figsize=(10, 7))  \n",
    "plt.title(\"Dendrograms\")  \n",
    "dend = shc.dendrogram(shc.linkage(X, method='ward'))\n",
    "plt.axhline(y=50, color='r', linestyle='--')\n",
    "\n",
    "\n",
    "# Agglomaerative clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "cluster = AgglomerativeClustering( n_clusters= 3 , affinity='euclidean', linkage='ward')  \n",
    "cluster_labels = cluster.fit_predict(X)\n",
    "\n",
    "# affinity is how you calculate dist between two points.\n",
    "# linkage is how you find closeness between two clusters.\n",
    "\n",
    "# plot\n",
    "plt.scatter(X[:,0], X[:,1], c=cluster_labels, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ad99e2",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd54aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knee Finding in DBSCAN\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from matplotlib import pyplot as plt\n",
    "neigh = NearestNeighbors(n_neighbors=2)\n",
    "nbrs = neigh.fit(X)\n",
    "distances, indices = nbrs.kneighbors(X) #\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "plt.plot(distances)\n",
    "\n",
    "# till here you will get best eps\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = StandardScaler().fit_transform(X) # standardization\n",
    "\n",
    "# before finding the value of eps data must be in \"\"\"\"\"\"standardized form\"\"\"\"\"\".\n",
    "# transform() is used to transform the data\n",
    "# fit_transform() - train the model and also transform it afterwards.\n",
    "\n",
    "\n",
    "y_pred = DBSCAN(eps=0.3, min_samples=10).fit_predict(X)\n",
    "# fit_predict(), train\n",
    "# DBSCAN doesnt have predict function, since it can not predict for test data(new data).\n",
    "#you have to fit the new data to know the outliers\n",
    "\n",
    "import numpy as np\n",
    "# find the number of clusters\n",
    "print('Number of clusters: {}'.format(len(set(y_pred[np.where(y_pred != -1)]))))\n",
    "\n",
    "print('Outliers :', len(y_pred[np.where(y_pred == -1)]))\n",
    "# -1 means outliers\n",
    "\n",
    "# plot\n",
    "plt.scatter(X[:,0], X[:,1], c=y_pred, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9454539a",
   "metadata": {},
   "source": [
    "# KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49356195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"diamonds.csv\") #explore df\n",
    "\n",
    "# to remove unnamed:0 column use index_col = 0 inside read_csv\n",
    "df = pd.read_csv(\"diamonds.csv\",index_col =0) # explore df\n",
    "\n",
    "df_ohe = pd.get_dummies(df) #explore df_ohe\n",
    "\n",
    "# training and testing split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df_ohe\n",
    "\n",
    "# we only have X and not y(target variable)\n",
    "X_train,X_test = train_test_split(X,test_size = 0.3, random_state = 7)\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# try various values of k from 2 to 10\n",
    "inertia_list = []\n",
    "for k in range(2,11):\n",
    "    km = KMeans(n_clusters = k,random_state = 7)\n",
    "    km.fit(X_train)\n",
    "    clust_nos = km.predict(X_test)\n",
    "    \n",
    "    inertia_list.append(km.inertia_)\n",
    "# Elbow graph\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(range(2,11),inertia_list)\n",
    "# if you see a great bend at any value then that k value is best\n",
    "\n",
    "\n",
    "# check which k value is best\n",
    "s_score_dict = {}\n",
    "for k in range(2,11):\n",
    "    km = KMeans(n_clusters = k,random_state = 7)\n",
    "    km.fit(X_train)\n",
    "    clust_nos = km.predict(X_test)\n",
    "\n",
    "    s = silhouette_score(X_test,clust_nos,random_state = 7)\n",
    "    s_score_dict[k] = s\n",
    "print(s_score_dict)\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# Perfom KMeans clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create object of the model\n",
    "km = KMeans(n_clusters = 3,random_state = 7)\n",
    "# Train the model using fit()\n",
    "km.fit(X_train)\n",
    "km.cluster_centers_# 3 centroids and each centroid will have 27 dimensions(columns).\n",
    "\n",
    "# Get the predictions from the model using predict\n",
    "clust_nos = km.predict(X_test) # final prediction of classes\n",
    "\n",
    "# silhouette score\n",
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_score(X_test,clust_nos,random_state = 7)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# plot\n",
    "plt.scatter(X[:, 0], X[:, 1], c=clust_nos, s=50, cmap='viridis')\n",
    "centers = km.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee5365a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32aec0d7",
   "metadata": {},
   "source": [
    "# Outlier Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaf06ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = df.select_dtypes(exclude='object').columns\n",
    "\n",
    "## Z-score based / Standard Scaler based method\n",
    "# -----------------------\n",
    "def zscore_for_column(df,col):\n",
    "    #zscore = (val - mean) / std_dev\n",
    "    zscore = (df[col] - df[col].mean())/df[col].std()\n",
    "    return zscore\n",
    "\n",
    "\n",
    "# calculating zscore of each column and updating it in the copy.\n",
    "df_std = df_ohe.copy()\n",
    "for col in num_columns:\n",
    "    df_std[col] = zscore_for_column(df,col)\n",
    "    \n",
    "df_std.describe()\n",
    "\n",
    "\n",
    "# OR\n",
    "\n",
    "# Z score / Standardscaler using library\n",
    "df_std =df_ohe.copy()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sd = StandardScaler()\n",
    "sd.fit(df_std.loc[:,num_columns]) # it will learn --> mean and std_dev\n",
    "\n",
    "# acually apply the formul and convert the data --> transform\n",
    "df_std.loc[:,num_columns]=sd.transform(df_std.loc[:,num_columns])\n",
    "\n",
    "df_std.describe()\n",
    "\n",
    "# ---------------------------------\n",
    "\n",
    "# Replacing the outliers with -3 and 3 (extreme values)\n",
    "\n",
    "from IPython.utils.text import columnize\n",
    "def outlier_imputation(df,col):\n",
    "    print(\"number of outliers in \",col,end=\" \")\n",
    "    print((df.loc[df[col]< -3,col]).count()+(df.loc[df[col] > 3 ,col]).count())\n",
    "    #print(df.loc[(df[col] < -3) | (df[col] > +3), :].shape[0])\n",
    "    df.loc[df[col] < -3 ,col] = -3 \n",
    "    df.loc[df[col] > 3 ,col] = 3\n",
    "    return df\n",
    "\n",
    "\n",
    "from IPython.utils.text import columnize\n",
    "def outlier_imputation(df,col):\n",
    "    print(\"No of outliers in \", col , \" are\" , df.loc[(df[col] < -3) | (df[col] > +3), :].shape[0])\n",
    "    df.loc[df[col] < -3 ,col] = -3\n",
    "    df.loc[df[col] > 3 ,col] = 3\n",
    "    return df\n",
    "\n",
    "for col in num_columns:\n",
    "    df_std = outlier_imputation(df_std,col)\n",
    "    \n",
    "    \n",
    "# --------------------------------------------\n",
    "\n",
    "########################################################################\n",
    "# IQR Based outlier handling\n",
    "df_iqr = df_ohe.copy()\n",
    "q1, q3 = df['table'].quantile([0.25,0.75])\n",
    "def outlier_imputation_IQR(df,col):\n",
    "    q1, q3 = df[col].quantile([0.25,0.75])\n",
    "    iqr = q3 -q1\n",
    "\n",
    "    df.loc[df[col] < (q1-1.5*iqr),col ] = (q1-1.5*iqr) \n",
    "    df.loc[df[col] > (q3+1.5*iqr),col ] = (q3+1.5*iqr)\n",
    "    return df\n",
    "\n",
    "for col in num_columns:\n",
    "    df_iqr = outlier_imputation_IQR(df_iqr,col)\n",
    "\n",
    "df_iqr.describe()\n",
    "\n",
    "########################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f73879",
   "metadata": {},
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd256b7",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce598073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'alpha': [0.01,0.5,1,5],\n",
    "    'max_iter':[100,500,1000,5000]\n",
    "}\n",
    "\n",
    "# Ridge regression using grid search\n",
    "gscv = GridSearchCV(Ridge(random_state=7),param_grid,\n",
    "                scoring='r2',cv=2,verbose=2)\n",
    "\n",
    "gscv.fit(X_train,Y_train)\n",
    "\n",
    "print(gscv.best_score_)\n",
    "\n",
    "print(gscv.best_params_)\n",
    "\n",
    "# Create model with best parameters\n",
    "rg = Ridge(alpha=0.01, max_iter=100, random_state=7)\n",
    "Y_pred= rg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(r2_score(Y_test,Y_pred))\n",
    "\n",
    "# Predict on new data\n",
    "X_test.iloc[0:1,:]\n",
    "log_price = rg.predict(X_test.iloc[0:1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed8c6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52e10dc6",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded68139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c121208",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators':[50, 200] , \n",
    "    'max_samples': [0.6, 0.8]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f74709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv = GridSearchCV(RandomForestClassifier(max_depth = 4,\n",
    "                                           oob_score=True,\n",
    "                                           class_weight='balanced',\n",
    "                                           random_state=7), \n",
    "                    param_grid, cv=2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f68885",
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.fit(X_train_pca,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8fc766",
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ab8a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4093eb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create RF  model using best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee3d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators = 200,\n",
    "                       max_samples= 0.6,\n",
    "                        max_depth = 4,\n",
    "                        oob_score=True,\n",
    "                        class_weight='balanced',\n",
    "                        random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfcd665",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X_train_pca,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f460cba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = rfc.predict(X_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8468bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d47ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    \"C\" : [0.1, 2],\n",
    "    \"gamma\" : [0.5, 1]\n",
    "}\n",
    "\n",
    "gscv = GridSearchCV(SVC(random_state=7), param_grid, cv=2, verbose=2)\n",
    "\n",
    "\n",
    "gscv.fit(X_train_pca,y_train)\n",
    "gscv.best_score_\n",
    "gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e33ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77621c05",
   "metadata": {},
   "source": [
    "# Simple logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026aa0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"BankChurners_cleaned.csv\")\n",
    "\n",
    "X = df.drop(\"Attrition_Flag\",axis = 1)\n",
    "Y = df[\"Attrition_Flag\"]\n",
    "\n",
    "X_ohe = pd.get_dummies(X)\n",
    "\n",
    "Y.value_counts()\n",
    "\n",
    "Y.loc[Y=='Existing Customer'] = 0\n",
    "Y.loc[Y=='Attrited Customer'] = 1\n",
    "\n",
    "Y.value_counts()\n",
    "Y.dtype\n",
    "Y = Y.astype(\"int\")\n",
    "Y.dtype\n",
    "\n",
    "# Train Test Split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_ohe,Y,test_size = 0.3 ,random_state = 7,stratify = Y)\n",
    "\n",
    "# Apply Logistic regression / logit classifier\n",
    "###### class sklearn.linear_model.LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)[source]¶\n",
    "\n",
    "#n_jobs = if you want to run multiclass classifier, or how many threads you want to run\n",
    "#l1_ratio\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty = 'elasticnet',class_weight = 'balanced',random_state = 7, solver = 'saga',max_iter = 100,l1_ratio = 0.5)\n",
    "# elasticnet = 0.5l1 +0.5l2\n",
    "\n",
    "lr.fit(X_train,Y_train)\n",
    "\n",
    "# convergenceWarning = loss is not minimum, till now the best boundary is not found\n",
    "# model convergence = convergence = means achieving optimial soln. here achieving minimum loss.\n",
    "\n",
    "Y_pred = lr.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "## confusion matrix\n",
    "### sklearn.metrics.confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(Y_test,Y_pred)\n",
    "\n",
    "# true negative false positive\n",
    "# false negative true positive\n",
    "\n",
    "# true positive is 0 becuse our model is not converged yet.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9236a91",
   "metadata": {},
   "source": [
    "# End to End classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f93982c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Check The Data given by Competetion\n",
    "#training data : all columns\n",
    "#Consider training data as full data and train test split on it\n",
    "#test data : target columns missing : can't be used to evaluate our model\n",
    "\n",
    "train_df = pd.read_csv(\"train_jRxnrHD.csv\")\n",
    "train_df.info()\n",
    "test_df = pd.read_csv(\"test_QaJU1Mh.csv\")\n",
    "test_df.info()\n",
    "\n",
    "# Split the data\n",
    "\n",
    "data = train_df.copy()\n",
    "\n",
    "## Here target is in column 12 'target'\n",
    "#Premium is another target column which can be predicted for regression problem / multi-class problem\n",
    "\n",
    "X = data.drop(['premium', 'target'], axis=1)\n",
    "y = data.loc[:,'target']\n",
    "\n",
    "X.shape, y.shape\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,\n",
    "                 test_size=0.3,\n",
    "                 random_state=7,\n",
    "                 stratify=y)\n",
    "\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape\n",
    "\n",
    "y.value_counts(normalize=True)\n",
    "y_train.value_counts(normalize=True)\n",
    "y_test.value_counts(normalize=True)\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "#- Remove the id columns or columns with unique values \n",
    "#- here remove id column\n",
    "#- Remove columns which contain ONLY one value\n",
    "#- Remove columns / rows containing NAs more than threshold\n",
    "#- Fill missing values with median\n",
    "#- Encode the string columns\n",
    "\n",
    "\n",
    "## Remove the id columns or columns with unique values \n",
    "\n",
    "drop_columns = ['id']\n",
    "X_train.drop(drop_columns,axis=1,inplace=True)\n",
    "\n",
    "## columns which contain ONLY one value\n",
    "\n",
    "#NO columns \n",
    "\n",
    "X_train.nunique()\n",
    "\n",
    "## columns / rows containing NAs more than threshold\n",
    "\n",
    "#NO columns\n",
    "\n",
    "X_train.isna().sum()/ X_train.shape[0]\n",
    "\n",
    "## Fill missing values with median\n",
    "\n",
    "X_train.median()\n",
    "X_train.mode() #it will be used if na values are there in string type columns\n",
    "\n",
    "fill_value = X_train.median()\n",
    "X_train.fillna(fill_value, inplace=True)\n",
    "\n",
    "# Encode the string columns\n",
    "\n",
    "#- ONE hot encoding\n",
    "\n",
    "X_train.select_dtypes(include='object').columns\n",
    "X_train = pd.get_dummies(X_train)\n",
    "X_train.shape\n",
    "X_train.columns\n",
    "\n",
    "## Standardization\n",
    "\n",
    "##ONLY on numeric columns and not for categorical columns\n",
    "\n",
    "cat_columns = ['sourcing_channel', 'residence_area_type']\n",
    "num_columns = ['perc_premium_paid_by_cash_credit', 'age_in_days', 'Income',\n",
    "       'Count_3-6_months_late', 'Count_6-12_months_late',\n",
    "       'Count_more_than_12_months_late', 'application_underwriting_score',\n",
    "       'no_of_premiums_paid']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std = StandardScaler()\n",
    "std.fit(X_train.loc[:, num_columns])\n",
    "\n",
    "### Method 1 (Try to avoid)\n",
    "\n",
    "trans_data = std.transform(X_train.loc[:, num_columns])\n",
    "X_train_trans = pd.DataFrame(trans_data,\\\n",
    "                             columns = num_columns,\\\n",
    "                             index = X_train.index)\n",
    "\n",
    "ohe_columns = ['sourcing_channel_A', 'sourcing_channel_B',\n",
    "       'sourcing_channel_C', 'sourcing_channel_D', 'sourcing_channel_E',\n",
    "       'residence_area_type_Rural', 'residence_area_type_Urban']\n",
    "\n",
    "cat_df = X_train.loc[:, ['sourcing_channel_A', 'sourcing_channel_B',\n",
    "       'sourcing_channel_C', 'sourcing_channel_D', 'sourcing_channel_E',\n",
    "       'residence_area_type_Rural', 'residence_area_type_Urban']]\n",
    "\n",
    "X_train_final = pd.concat([X_train_trans, cat_df], axis=1)\n",
    "X_train_final.shape, X_train.shape\n",
    "\n",
    "### Another syntax  (EASY) ###########\n",
    "\n",
    "X_train_sc = X_train.copy()\n",
    "std.fit(X_train_sc.loc[:, num_columns])\n",
    "X_train_sc.loc[:, num_columns] = std.transform(X_train_sc.loc[:, num_columns])\n",
    "X_train_sc.shape, X_train.shape\n",
    "\n",
    "\n",
    "\n",
    "# Outlier Handling\n",
    "#- impute the outliers in numeric columns using IQR method\n",
    "\n",
    "#IQR = Q3 - Q1\n",
    "#min_val_val = Q1 - 1.5 IQR\n",
    "#max_val_val = Q3 + 1.5 IQR\n",
    "\n",
    "def impute_IQR(df, col):\n",
    "    q1,q3 = df[col].quantile([0.25, 0.75])\n",
    "    iqr = q3-q1\n",
    "    min_val_val = q1 - 1.5 * iqr\n",
    "    max_val_val = q3 + 1.5 * iqr\n",
    "    df.loc[df[col] < min_val_val , col] = min_val_val\n",
    "    df.loc[df[col] > max_val_val , col] = max_val_val\n",
    "    return df\n",
    "\n",
    "for col in num_columns:\n",
    "    X_train_sc = impute_IQR(X_train_sc,col)\n",
    "\n",
    "X_train_sc.describe()\n",
    "\n",
    "# Feature Selection\n",
    "\n",
    "#- PCA\n",
    "#- RFE\n",
    "#- SelectFromModel ( Decision Tree)\n",
    "\n",
    "## PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 0.99, random_state=7)\n",
    "pca.fit(X_train_sc)\n",
    "X_train_pca = pca.transform(X_train_sc)\n",
    "X_train_pca.shape\n",
    "\n",
    "# Train the model\n",
    "\n",
    "#- SVM\n",
    "#fine tune C and gamma parameters\n",
    "#- RandomForest\n",
    "# n_estimators , max_samples , max_depth needs to be tuned\n",
    "#- CatBoost\n",
    "\n",
    "## SVM classifier ( SVC)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    \"C\" : [0.1, 2],\n",
    "    \"gamma\" : [0.5, 1]\n",
    "}\n",
    "\n",
    "gscv = GridSearchCV(SVC(random_state=7), param_grid, cv=2, verbose=2)\n",
    "gscv.fit(X_train_pca,y_train)\n",
    "gscv.best_score_\n",
    "gscv.best_params_\n",
    "\n",
    "## RandomForest \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "param_grid = {\n",
    "    'n_estimators':[50, 200] , \n",
    "    'max_samples': [0.6, 0.8]\n",
    "}\n",
    "\n",
    "gscv = GridSearchCV(RandomForestClassifier(max_depth = 4,\n",
    "                                           oob_score=True,\n",
    "                                           class_weight='balanced',\n",
    "                                           random_state=7), \n",
    "                    param_grid, cv=2, verbose=2)\n",
    "\n",
    "gscv.fit(X_train_pca,y_train)\n",
    "\n",
    "gscv.best_score_\n",
    "gscv.best_params_\n",
    "\n",
    "### Create RF  model using best params\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators = 200,\n",
    "                       max_samples= 0.6,\n",
    "                        max_depth = 4,\n",
    "                        oob_score=True,\n",
    "                        class_weight='balanced',\n",
    "                        random_state=7)\n",
    "\n",
    "rfc.fit(X_train_pca,y_train)\n",
    "\n",
    "### predict on train data only because test data is not preprocessed\n",
    "\n",
    "y_pred_train = rfc.predict(X_train_pca)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_train, y_pred_train)\n",
    "\n",
    "# Make Test Data Ready fore predictions\n",
    "#same steps of preprocessing as that of training data are done \n",
    "#same features are selected here\n",
    "\n",
    "\n",
    "X_test.drop(drop_columns, axis=1,inplace=True)\n",
    "X_test.fillna(fill_value, inplace=True)\n",
    "X_test_ohe = pd.get_dummies(X_test)\n",
    "#X_test_ohe.shape, X_test.shape\n",
    "X_test_ohe.loc[:, num_columns] = std.transform(X_test_ohe.loc[:, num_columns])\n",
    "#X_test_ohe.shape\n",
    "X_test_ohe.describe()\n",
    "\n",
    "### feature selection using PCA for test data\n",
    "\n",
    "X_test_pca = pca.transform(X_test_ohe)\n",
    "#X_test_pca.shape\n",
    "\n",
    "# prediction on Test data\n",
    "\n",
    "y_pred = rfc.predict(X_test_pca)\n",
    "\n",
    "f1_score(y_test,y_pred)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "# Apply the model to predict on new data\n",
    "#- apply all the preprocessing steps\n",
    "#- apply feature selection steps\n",
    "\n",
    "\n",
    "rfc.predict(X_test_pca[:1])\n",
    "rfc.predict_proba(X_test_pca[:1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
